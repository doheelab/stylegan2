# -*- coding: utf-8 -*-
"""Toonify yourself의 빵형
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1WyUoMjpiTyOWt52KgwCBhzY1itDhUgvF
# Toonify yourself!
Please ensure that you're using a GPU runtime
First some setup:
"""


# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Commented out IPython magic to ensure Python compatibility.
# !git clone https://github.com/justinpinkney/stylegan2
# %cd stylegan2

import os 


!nvcc test_nvcc.cu -o test_nvcc -run

!mkdir raw
!mkdir aligned
!mkdir generated

"""## Upload your own photos
Upload your photos to `raw/`. These don't need to be aligned as we'll use a face detector to grab all the faces and transform them into the correct format. One note of caution is that you'll need a pretty high-resolution picture of a face to get a sharp result (the final face crop is resized to 1024x1024 pixels)
We'll grab a example image from the internet to work with.
The basic process is:
- Extract faces and align the images
- Project the images (i.e. find the latent code)
- Toonify the images (i.e. use the latent code with the toon model)
Results will be placed in the stylegan2/generated folder
"""

!wget https://upload.wikimedia.org/wikipedia/commons/6/6d/Shinz%C5%8D_Abe_Official.jpg -O raw/example.jpg
!wget https://upload.wikimedia.org/wikipedia/commons/9/95/191215_TVN_%EC%A6%90%EA%B1%B0%EC%9B%80%EC%A0%84_%ED%98%B8%ED%85%94%EB%8D%B8%EB%A3%A8%EB%82%98_%ED%86%A0%ED%81%AC%EC%84%B8%EC%85%98_%EC%95%84%EC%9D%B4%EC%9C%A0_%286%29.jpg -O raw/iu_01.jpg
!wget https://image.bugsm.co.kr/artist/images/1000/800491/80049126.jpg -O raw/iu_02.jpg

"""# Load Pretrained Models"""

import pretrained_networks 

# use my copy of the blended model to save Doron's download bandwidth
# get the original here https://mega.nz/folder/OtllzJwa#C947mCCdEfMCRTWnDcs4qw
blended_url = "https://drive.google.com/uc?id=1H73TfV5gQ9ot7slSed_l-lim9X7pMRiU" 
ffhq_url = "http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-ffhq-config-f.pkl"

_, _, Gs_blended = pretrained_networks.load_networks(blended_url)
_, _, Gs = pretrained_networks.load_networks(ffhq_url)

"""# Align Faces"""

!python align_images.py raw aligned

"""# Extract Latent Vector from Aligned Face"""

!python project_images.py --num-steps 1000 aligned generated

"""# Get Result from Blended Model"""

import numpy as np
from PIL import Image
import dnnlib
import dnnlib.tflib as tflib
from pathlib import Path

latent_dir = Path("generated")
latents = latent_dir.glob("*.npy")


for latent_file in latents:
  latent = np.load(latent_file)
  latent = np.expand_dims(latent,axis=0)
  synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=False), minibatch_size=8)
  images = Gs_blended.components.synthesis.run(latent, randomize_noise=False, **synthesis_kwargs)
  Image.fromarray(images.transpose((0,2,3,1))[0], 'RGB').save(latent_file.parent / (f"{latent_file.stem}-toon.jpg"))

from PIL import Image
import matplotlib.pyplot as plt

embedded = Image.open("generated/example_01.png")
tooned = Image.open("generated/example_01-toon.jpg")

plt.figure(figsize=(16, 10))
plt.imshow(np.concatenate([np.array(embedded), np.array(tooned)], axis=1))

embedded = Image.open("generated/iu_01_01.png")
tooned = Image.open("generated/iu_01_01-toon.jpg")

plt.figure(figsize=(16, 10))
plt.imshow(np.concatenate([np.array(embedded), np.array(tooned)], axis=1))

embedded = Image.open("generated/iu_02_01.png")
tooned = Image.open("generated/iu_02_01-toon.jpg")

plt.figure(figsize=(16, 10))
plt.imshow(np.concatenate([np.array(embedded), np.array(tooned)], axis=1))
